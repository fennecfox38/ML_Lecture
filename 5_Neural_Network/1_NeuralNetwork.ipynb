{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network : 신경망 모형\n",
    "\n",
    "신경망 모형은 Warren McCulloch와 Walter Pitts가 (1943) 인간의 뇌를 수학적 모형으로 표현하여 인간처럼 판단을 수행하고자 하는 아이디어로 출발하였다.\n",
    "\n",
    "이 아이디어는 $f(\\sum(w_ix_i+b))$의 값이 y 값이 되도록 미지수 w 값을 구하고자 하는 것이다.\n",
    "\n",
    "![NeuronCell.png](NeuronCell.png)\n",
    "![NeuralModel.png](NeuralModel.png)\n",
    "\n",
    "입력 X1, X2, X3 값에 각 가중치를 곱하는데 가중치가 크다는 것은 해당 자극이 신경을 활성화하는데중요한 자극이라는 의미이다. 즉, 원하는 결과가 어떤 자극에 의해 나타나는지를 가중치로 구하는 것이다. 이때 f는 활성함수(Activation function)이라고 한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "신경망 모형에서 활성 함수를 Sigmoid를 사용할 경우, 신경망 모형은 Logistic Regression(Cox,1958) 모형이 된다. 서로 접근방법은 달랐지만 McCulloch-Pitts의 신경망 모형과 Cox의 로지스틱 회귀는 결국 같은 모형이다.\n",
    "\n",
    "y는 종속변수이고, X1, X2, X3는 독립변수이다.\n",
    "일반적으로 신경망에서는 X1~X3를 Feature, y를 hypothesis, t는 target 이라고 부른다.\n",
    "\n",
    "![](NeuralModel.png)\n",
    "\n",
    "$$Y=S(w_1x_1+w_2x_2+w_3x_3+b)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$=\\frac{1}{1+exp(-(w_1x_1+w_2x_2+w_3x_3+b))}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "초기 신경망 모형은 Linear 한 모형으로 or, and 는 풀 수 있으나 xor는 풀 수 없어 많은 사람들이 xor문제에 집중하였다. 이후, 1969년 Minsky는 Hidden Layer를 사용하는 Multiple layer perceptron을 사용해 xor문제를 풀 수 있음을 증명했는데 문제는 MLP의 가중치를 구할 수 없다는 문제에 직면했다.\n",
    "\n",
    "![or_and_xor.png](or_and_xor.png)\n",
    "\n",
    "이후, 1974년 Backpropagation 알고리즘이 나와 MLP의 가중치를 구할 수 있게 되었으나 그 당시 주목받지 못하다가 1986년 Hinton에 다시 동일한 내용의 논문을 발표하면서 부터 다시 주목받기 시작했다. 이후, 10년 정도 Neural Network이 유행했으나 많은 단점을 노출하면서 다시 시들해 졌다.\n",
    "단점으로는 대표적으로 Over-fitting과 layer의 수가 커질수록 학습이 안되는 문제가 있다.\n",
    "2006, 2007년에 문제에 대한 솔루션이 나오면서 신경망 모형은 다시 Deep Learning이란 이름으로 Re-Branding하였다. 이후, 빅데이터 등 호재를 만나면서 특히, Image인식 분야에서 본격적으로 딥러닝의 시대를 열었다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 초기 신경망 모형\n",
    "* $W$: Weight (가중치)\n",
    "* $Y=S(WX+B)$: hypothesis by sigmoid (예측값; 예측한 값)\n",
    "* $target$: target (예측대상)\n",
    "* $error=Y-target$: 오차\n",
    "* $$W := W - X^T \\cdot error \\cdot S'(Y)$$\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# OR Operation using early neural network model\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    e=np.exp(x)\n",
    "    return (-e/(1+e))\n",
    "\n",
    "X = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
    "W = np.array([-0.99,-0.16,0.44]).reshape(3,1)\n",
    "target = (X[:,1]|X[:,2]).reshape(4,1)\n",
    "\n",
    "for i in range (1000):\n",
    "    Y = sigmoid(np.dot(X,W))\n",
    "    error = Y-target\n",
    "    W += np.dot(X.T, error*deriv_sigmoid(Y))\n",
    "\n",
    "print(Y)\n",
    "print(target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01000965]\n",
      " [0.99724653]\n",
      " [0.99724627]\n",
      " [0.99999992]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perceptron (rosenblatt, 1953)\n",
    "* $w_i$: Weight (가중치)\n",
    "* $y=\n",
    "\\begin{cases} \n",
    "    0 & \\,\\, (W^TX+b<=0)\\\\\n",
    "    1 & \\,\\, (W^TX+b>0)\n",
    "\\end{cases}$\n",
    "* $f(net)$: y value calculated through network\n",
    "* $t-f(net)$: error\n",
    "* $\\eta$: learning rate\n",
    "* $$w_i := w_i + \\eta x_i (t-f(net))$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# AND Operation using Perceptron\n",
    "import numpy as np\n",
    "\n",
    "def fnet(Y):\n",
    "    for i in range(4):\n",
    "        Y[i] = 1 if Y[i]>0 else 0\n",
    "    return Y\n",
    "\n",
    "X = np.array([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
    "W = np.array([-0.99,-0.16,0.44]).reshape(3,1)\n",
    "T = (X[:,1]&X[:,2]).reshape(4,1)\n",
    "eta = 0.1\n",
    "\n",
    "for i in range (10):\n",
    "    Y = np.dot(X,W)\n",
    "    W += eta * np.dot(X.T, T - fnet(Y))\n",
    "    print(i,\": | Y =\",Y.reshape(-1))\n",
    "print(Y.reshape(-1))\n",
    "print(T.reshape(-1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 : | Y = [0. 0. 0. 0.]\n",
      "1 : | Y = [0. 0. 0. 0.]\n",
      "2 : | Y = [0. 0. 0. 0.]\n",
      "3 : | Y = [0. 1. 0. 1.]\n",
      "4 : | Y = [0. 0. 0. 0.]\n",
      "5 : | Y = [0. 1. 0. 1.]\n",
      "6 : | Y = [0. 0. 0. 1.]\n",
      "7 : | Y = [0. 0. 0. 1.]\n",
      "8 : | Y = [0. 0. 0. 1.]\n",
      "9 : | Y = [0. 0. 0. 1.]\n",
      "[0. 0. 0. 1.]\n",
      "[0 0 0 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implement Hidden Layer\n",
    "\n",
    "The deep learning model is a model that extends the hidden layer to two or more in the neural network model.\n",
    "\n",
    "![HiddenLayer1.png](HiddenLayer1.png)\n",
    "\n",
    "With $n$ of given datasets,\n",
    "\n",
    "$size(X)= n \\times n_X$<br>\n",
    "$size(W_1)= n_X \\times n_{H_1}$<br>\n",
    "$size(H_1)= n \\times n_{H_1}$<br>\n",
    "$size(W_2)= n_{H_1} \\times n_{H_2}$<br>\n",
    "$size(H_2)= n \\times n_{H_2}$<br>\n",
    "$size(W_3)= n_{H_2} \\times 1$<br>\n",
    "$size(Y)= n \\times 1$\n",
    "\n",
    "XOR Operation realization was impossible with early neural network model. But it can be realized by **implementing hideen layer**.\n",
    "\n",
    "XOR Operation\n",
    "* has 2 inputs and 1 output.\n",
    "* Assume that it has 1 hidden layer with 3 nodes.\n",
    "* then $W_1$ is 2x3 matrix (consist of 6 wires)\n",
    "* and $W_2$ is 3x1 matrix (consist of 3 wires)\n",
    "* $H=X \\cdot W_1+B_1$\n",
    "* $Y=H \\cdot W_2+B_2$\n",
    "\n",
    "![HiddenLayer2.png](HiddenLayer2.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# XOR Operation using Hidden Layer (with given proper bias and weight)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "X=np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "W1=np.array([[-2, 5, 4], [ 3, 6, 3]])\n",
    "B1=np.array([2, -2, -5])\n",
    "W2=np.array([[-4], [ 8], [-8]])\n",
    "H=Sigmoid(np.dot(X,W1)+B1)\n",
    "Y=Sigmoid(np.dot(H,W2))\n",
    "\n",
    "print (Y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.06766597]\n",
      " [0.94927397]\n",
      " [0.96979454]\n",
      " [0.0542867 ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then XOR Design Problem falls into finding 'Weight $W$'\n",
    "Proper weight can be found by **Backpropagation**\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "Find the proper Weight $W$ and Bias $B$ for XOR Designed above.\n",
    "For $g(x)=\\frac{1}{1+e^{-x}}$, define as $H=g(X \\cdot W_1 + B_1)$, $Y=g(H \\cdot W_2)$, $T: Target Value$\n",
    "\n",
    "The matrix transformation and Cost function are as follows: \n",
    "\n",
    "\n",
    "$$X \\xrightarrow{W_1} H \\xrightarrow{W_2} Y, \\quad Cost = \\frac{1}{2}(Y-T)^2$$\n",
    "\n",
    "\n",
    "* $X$: $n \\times 2$\n",
    "* $W_1$: $2 \\times 3$\n",
    "* $H$: $n \\times 3$\n",
    "* $W_2$: $3 \\times 1$\n",
    "* $Y$: $1 \\times 1$\n",
    "\n",
    "Considering **Gradient Descent Method**, Weight $W_1$ and $W_2$ follow\n",
    "\n",
    "\n",
    "$$W_1 := W_1 - \\lambda \\frac{\\partial}{\\partial W_1} Cost$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$W_2 := W_2 - \\lambda \\frac{\\partial}{\\partial W_2} Cost$$\n",
    "\n",
    "\n",
    "(Note that $W^+$ is updated W)\n",
    "\n",
    "(remind: $\\frac{d}{dY}Cost = Y-T$, $\\frac{d}{dx}g(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = g(x) \\cdot (1-g(x))$)\n",
    "\n",
    "(Keep in mind that we should check sizes of matrices in every single step in order to check validity of matrix product)\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "$\\frac{\\partial Cost}{\\partial W_2}$ can be solved as:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial Cost}{\\partial W_2} = \\frac{d Cost}{dY} \\cdot \\frac{\\partial Y}{\\partial W_2} = (Y-T) \\frac{\\partial}{\\partial W_2} g(H \\cdot W_2)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= (Y-T) g(H \\cdot W_2)(1-g(H \\cdot W_2)) \\frac{d (H \\cdot W_2)}{d W_2}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= H^T (Y-T) g(H \\cdot W_2)(1-g(H \\cdot W_2))$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= H^T (Y-T) Y(1-Y)$$\n",
    "\n",
    "\n",
    "Define as $\\delta_Y := (Y-T) Y(1-Y)$, then\n",
    "\n",
    "\n",
    "$$\\frac{\\partial Cost}{\\partial W_2} = H^T \\delta_Y$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\therefore W_2 := W_2 - \\lambda H^T \\delta_Y$$\n",
    "\n",
    "\n",
    "and for $\\frac{\\partial Cost}{\\partial W_1}$,\n",
    "\n",
    "\n",
    "$$\\frac{\\partial Cost}{\\partial W_1} = \\frac{d Cost}{dY} \\cdot \\frac{\\partial Y}{\\partial W_1} = (Y-T) \\frac{\\partial}{\\partial W_1} g(H \\cdot W_2)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= (Y-T) \\cdot \\frac{\\partial g(H \\cdot W_2)}{\\partial (H \\cdot W_2)} \\cdot \\frac{\\partial (H \\cdot W_2)}{\\partial H} \\cdot \\frac{\\partial H}{\\partial W_1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= (Y-T) g(H \\cdot W_2) (1-g(H \\cdot W_2)) \\cdot {W_2}^T \\cdot \\frac{\\partial}{\\partial W_1} g(X \\cdot W_1 + B_1)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= (Y-T) Y (1-Y) \\cdot {W_2}^T \\cdot g(X \\cdot W_1 + B_1) (1-g(X \\cdot W_1 + B_1) )\\frac{\\partial(X \\cdot W_1 + B_1)}{\\partial W_1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$= X^T (Y-T) Y (1-Y) \\cdot {W_2}^T \\cdot H (1-H)$$\n",
    "\n",
    "\n",
    "Define as $\\delta_H := (Y-T) Y(1-Y) \\cdot {W_2}^T \\cdot H (1-H) = \\delta_Y \\cdot {W_2}^T \\cdot H (1-H) $, then\n",
    "\n",
    "\n",
    "$$\\frac{\\partial Cost}{\\partial W_1} = X^T \\delta_H$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\therefore W_1 := W_1 - \\lambda X^T \\delta_H$$\n",
    "\n",
    "\n",
    "Also, in simillar process, (proof omitted)\n",
    "\n",
    "\n",
    "$$B_1 := B_1 - \\lambda \\delta_H$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# XOR Operation using Hidden Layer (Finding bias and weight using Backpropagation)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "lamda = 1\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "T = np.array([[0],[1],[1],[0]])\n",
    "b1 = 2 * np.random.rand(1,3) - 1    # (0~1) -> (0~2) -> (-1~1)\n",
    "W1 = 2 * np.random.rand(2,3) - 1    # random in range (-1,1)\n",
    "W2 = 2 * np.random.rand(3,1) - 1\n",
    "\n",
    "for i in range(0,1000):\n",
    "    H = sigmoid(np.dot(X,W1)+b1)\n",
    "    Y = sigmoid(np.dot(H,W2))\n",
    "\n",
    "    deltaY = np.multiply(Y-T,np.multiply(Y,(1-Y)))\n",
    "    temp = np.multiply(W2.T,np.multiply(H,(1-H)))\n",
    "    deltaH = deltaY * temp\n",
    "\n",
    "    W2 -= np.dot(H.T,lamda*deltaY)\n",
    "    W1 -= np.dot(X.T, lamda*deltaH)\n",
    "    b1 = b1 - lamda*deltaH\n",
    "\n",
    "print(Y)\n",
    "print(\"b1: \",b1)\n",
    "print(\"W1: \",W1)\n",
    "print(\"W2: \",W2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01812534]\n",
      " [0.98257188]\n",
      " [0.98047371]\n",
      " [0.01830072]]\n",
      "b1:  [[-1.93444526  2.01417724 -0.99103309]\n",
      " [ 1.62960896 -2.86126629  0.45754984]\n",
      " [ 2.61223988 -1.98303732 -0.13467264]\n",
      " [-1.42536695  2.2972015  -0.05934399]]\n",
      "W1:  [[-0.62177544 -0.56411775 -0.61059158]\n",
      " [ 0.25692385  0.61102013  0.19016233]]\n",
      "W2:  [[ 4.58260684]\n",
      " [-5.44897151]\n",
      " [ 0.87246938]]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extension of Backpropagation (Multiple Hidden Layer)\n",
    "\n",
    "As we derived update equation of weight for XOR operation above, **Backpropagation** can be extended to multiple hidden layer.\n",
    "\n",
    "See below example for 3-hidden layer:\n",
    "\n",
    "\n",
    "$$X \\xrightarrow{W_1} H_1 \\xrightarrow{W_2} H_2 \\xrightarrow{W_3} H_3 \\xrightarrow{W_4} Y$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\quad \\quad \\quad \\delta_1 \\xleftarrow{\\quad} \\delta_2 \\xleftarrow{\\quad} \\delta_3 \\xleftarrow{\\quad} \\delta_Y$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\delta_Y = (Y-T) \\cdot Y(1-Y), \\quad W_4 := W_4 - \\lambda \\cdot {H_3}^T \\cdot \\delta_Y$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\delta_3 = \\delta_Y \\cdot {W_4}^T \\cdot H_3 (1-H_3), \\quad W_3 := W_3 - \\lambda \\cdot {H_2}^T \\cdot \\delta_3$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\delta_2 = \\delta_3 \\cdot {W_3}^T \\cdot H_2 (1-H_2), \\quad W_2 := W_2 - \\lambda \\cdot {H_1}^T \\cdot \\delta_2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\delta_1 = \\delta_2 \\cdot {W_2}^T \\cdot H_1 (1-H_1),  \\quad W_1 := W_1 - \\lambda \\cdot X^T \\cdot \\delta_1$$"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}