{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# XOR Operation using tensorflow.keras (1 hidden layer)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential() # Sequential Model\n",
    "model.add(layers.Dense(3, activation='sigmoid', input_dim=2)) # Output(H): 3 | Activation Function: Sigmoid | Input(X): 2\n",
    "model.add(layers.Dense(1, activation='sigmoid'))# Output(Y): 1 | Activation Function: Sigmoid | (Input(H): 3)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1) # Stochastic Gradient Descent | Learning Rate: Lambda = 0.1\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy',metrics=['accuracy']) # Loss Function (Cost Function) : Binary Cross Entropy\n",
    "\n",
    "model.fit(x, y, epochs=10000, batch_size=4, verbose=1) # 10000 epochs 4 batch -> 40000 updates"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000\n",
      "Epoch 9805/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9806/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9807/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9808/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9809/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9810/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9811/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9812/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9813/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 9814/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9815/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9816/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9817/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9818/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9819/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9820/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9821/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9822/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9823/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9824/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9825/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9826/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9827/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9828/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9829/10000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9830/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9831/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9832/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9833/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9834/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9835/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9836/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9837/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9838/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9839/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9840/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9841/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9842/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9843/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9844/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9845/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9846/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9847/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9848/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9849/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9850/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9851/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9852/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9853/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9854/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9855/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9856/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9857/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9858/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 9859/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9860/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9861/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9862/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9863/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9864/10000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9865/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9866/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9867/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9868/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9869/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9870/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9871/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9872/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9873/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9874/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9875/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9876/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9877/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9878/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9879/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9880/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9881/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9882/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9883/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9884/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9885/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9886/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9887/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9888/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9889/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9890/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9891/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9892/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9893/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9894/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9895/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9896/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9897/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9898/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9899/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9900/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9901/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9902/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9903/10000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 9904/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9905/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9906/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9907/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9908/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9909/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9910/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9911/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9912/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9913/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9914/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9915/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9916/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9917/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9918/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9919/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9920/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9921/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9922/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9923/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9924/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9925/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9926/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9927/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9928/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9929/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9930/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9931/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9932/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9933/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9934/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9935/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9936/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9937/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9938/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9939/10000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9940/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9941/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9942/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9943/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9944/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9945/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9946/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9947/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9948/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 9949/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9950/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9951/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9952/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9953/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9954/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9955/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9956/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9957/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9958/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9959/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9960/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9961/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9962/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9963/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9964/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9965/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9966/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9967/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9968/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9969/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9970/10000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9971/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9972/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9973/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9974/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9975/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9976/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9977/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9978/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9979/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9980/10000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9981/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9982/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9983/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9984/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9985/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9986/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9987/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9988/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9989/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9990/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9991/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9992/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9993/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9994/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 9995/10000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 9996/10000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 9997/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 9998/10000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 9999/10000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 10000/10000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0132 - accuracy: 1.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb880e68730>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model.evaluate(x, y)\n",
    "predicted = model.predict(x)\n",
    "print(predicted)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 402ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "[[0.01271769]\n",
      " [0.98754215]\n",
      " [0.98773026]\n",
      " [0.01515111]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XOR Operation using sigmoid function with 6 hidden layers.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "actFunc = 'sigmoid' # 학습이 안된다.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, activation=actFunc, input_dim=2))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(x, y, epochs=10000, batch_size=2, verbose=1)\n",
    "model.evaluate(x, y)\n",
    "\n",
    "predicted = model.predict(x)\n",
    "predicted"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Vanishingn Problem\n",
    "* is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.\n",
    "\n",
    "In those methods (Gradient Descent Methos and backpropagation), each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training.\n",
    "\n",
    "The problem is that in some cases, **the gradient will be vanishingly small**, **effectively preventing the weight from changing its value**.\n",
    "\n",
    "In the process of backpropagation, weight will be updated by partial derivative of sigmoid function. If the absolute value of $WX+B$, the input of sigmoid is too big, then its slope gets close to '0' and weight gets updated no longer. \n",
    "\n",
    "![VanishingGradient.png](VanishingGradient.png)\n",
    "\n",
    "**Improved activation function model required! (See below link)**\n",
    "\n",
    "[excelsior-cjh.tistory.com](https://excelsior-cjh.tistory.com/177)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ReLU Function (Rectified Linear Unit)\n",
    "$$ReLU(x)=max(0,x)$$\n",
    "\n",
    "ReLU Function bypasses positives and set negatives as zero.\n",
    "\n",
    "Since its slope is simply '0' or '1', the process of backpropagation gets a way easier. (less consuming resources)\n",
    "\n",
    "Dead ReLU Problem: ReLU returns 0 once sum of the weight goes to negative because its gradient is also 0. (this node is dead then.)\n",
    "\n",
    "# LeakyReLU Function\n",
    "$$LeakyReLU_{\\alpha}(x)=max(\\alpha x,x), \\quad \\alpha \\approx 0.01$$\n",
    "\n",
    "By implementing hyper-parameter $\\alpha$ (normally about 0.01),'Dead ReLU' is solved.\n",
    "\n",
    "# ELU Function (Exponential Linear Unit)\n",
    "$$\n",
    "ELU_{\\alpha} =\n",
    "\\begin{cases} \n",
    "    \\alpha e^x -1 & \\quad(x<0)\\\\\n",
    "    x & \\quad (x \\geq 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* When $x<0$, ELU return mean gets close to 0 so that bias shift decreases and Vanishing Gradeint problem decreases.\n",
    "* Even if $x<0$, gradient is non-zero so that dead neuron is not made.\n",
    "* Hyper parameter $\\alpha$ determines convergence value when x is negative.\n",
    "* When $\\alpha=1$, ELU has a continuous slope (differentiable) at $x=0$ and in range $(-\\infty,\\infty)$ therefore rate of convergence in GDM is faster.\n",
    "\n",
    "---\n",
    "\n",
    "Generally, activation functions are used more in order of ELU, LeakyReLU, tanh, sigmoid. cs231n recommends ReLU first, then ReLu Family(LeakyReLU or ELU) and avoid sigmoid.\n",
    "\n",
    "[cs231n.stanford.edu](http://cs231n.stanford.edu/)\n",
    "\n",
    "---\n",
    "\n",
    "Keep in mind that output layer must returns value between 0 and 1 for one-hot encoded categories or binary. **Use sigmoid for output layer if needed**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def ReLU(x) : \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def LeakyReLU(x,alpha) : \n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "def ELU(x,alpha) : \n",
    "    return (x>0)*x + (x<=0)*(alpha*(np.exp(x)-1))\n",
    "\n",
    "domain = np.linspace(-3,3,600)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Sigmoid')\n",
    "plt.plot(domain,Sigmoid(domain))\n",
    "plt.subplot(222)\n",
    "plt.title('ELU')\n",
    "plt.plot(domain,ELU(domain,1))\n",
    "plt.subplot(223)\n",
    "plt.title('ReLU')\n",
    "plt.plot(domain,ReLU(domain))\n",
    "plt.subplot(224)\n",
    "plt.title('LeakyReLU')\n",
    "plt.plot(domain,LeakyReLU(domain,0.1))\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ],
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 378.465625 263.63625\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-07-19T08:27:03.954428</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 378.465625 263.63625 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 121.154489 \nL 188.647443 121.154489 \nL 188.647443 22.318125 \nL 36.465625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m65a2461b9e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.440832\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −2 -->\n      <g transform=\"translate(59.069738 135.752926)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.556534\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0 -->\n      <g transform=\"translate(109.375284 135.752926)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.672237\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(155.490987 135.752926)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md6e7977b10\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"96.553026\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 100.352245)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"71.736307\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 75.535526)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"46.919588\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.75 -->\n      <g transform=\"translate(7.2 50.718807)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_7\">\n    <path clip-path=\"url(#paf56b18773)\" d=\"M 43.38298 116.661927 \nL 48.69514 115.514158 \nL 53.545372 114.241739 \nL 58.164641 112.797211 \nL 62.552947 111.184512 \nL 66.710289 109.414503 \nL 70.636668 107.505099 \nL 74.563046 105.346703 \nL 78.489425 102.924866 \nL 82.415804 100.229936 \nL 86.342182 97.258721 \nL 90.499524 93.817173 \nL 94.88783 89.873855 \nL 99.507099 85.41792 \nL 105.050222 79.744605 \nL 113.364906 70.866361 \nL 122.372481 61.328306 \nL 127.68464 56.014191 \nL 132.303909 51.692965 \nL 136.692215 47.897438 \nL 140.849557 44.606573 \nL 144.775936 41.781566 \nL 148.702314 39.232062 \nL 152.628693 36.951239 \nL 156.555072 34.926702 \nL 160.48145 33.142108 \nL 164.638792 31.49313 \nL 169.027098 29.995153 \nL 173.646367 28.656967 \nL 178.496599 27.481017 \nL 181.730088 26.810687 \nL 181.730088 26.810687 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 121.154489 \nL 36.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 188.647443 121.154489 \nL 188.647443 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 121.154489 \nL 188.647443 121.154489 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 22.318125 \nL 188.647443 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- Sigmoid -->\n    <g transform=\"translate(88.279972 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"91.259766\" xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"154.736328\" xlink:href=\"#DejaVuSans-6d\"/>\n     <use x=\"252.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"313.330078\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"341.113281\" xlink:href=\"#DejaVuSans-64\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 219.083807 121.154489 \nL 371.265625 121.154489 \nL 371.265625 22.318125 \nL 219.083807 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.059013\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −2 -->\n      <g transform=\"translate(241.68792 135.752926)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.174716\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(291.993466 135.752926)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.290418\" xlink:href=\"#m65a2461b9e\" y=\"121.154489\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g transform=\"translate(338.109168 135.752926)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"95.048456\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(205.721307 98.847675)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"49.55661\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2 -->\n      <g transform=\"translate(205.721307 53.355829)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p2a593fe83d)\" d=\"M 226.001162 116.661927 \nL 235.470664 116.086811 \nL 243.323421 115.393973 \nL 250.021361 114.584857 \nL 255.795447 113.671551 \nL 260.876643 112.655131 \nL 265.495912 111.515191 \nL 269.653255 110.274567 \nL 273.579633 108.878579 \nL 277.275048 107.328796 \nL 280.7395 105.632061 \nL 283.972989 103.801113 \nL 286.975513 101.854992 \nL 289.978038 99.638212 \nL 292.7496 97.319257 \nL 295.521161 94.706698 \nL 364.34827 26.810687 \nL 364.34827 26.810687 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 219.083807 121.154489 \nL 219.083807 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 371.265625 121.154489 \nL 371.265625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 219.083807 121.154489 \nL 371.265625 121.154489 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 219.083807 22.318125 \nL 371.265625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- ELU -->\n    <g transform=\"translate(283.949091 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 556 4666 \nL 1191 4666 \nL 1191 1831 \nQ 1191 1081 1462 751 \nQ 1734 422 2344 422 \nQ 2950 422 3222 751 \nQ 3494 1081 3494 1831 \nL 3494 4666 \nL 4128 4666 \nL 4128 1753 \nQ 4128 841 3676 375 \nQ 3225 -91 2344 -91 \nQ 1459 -91 1007 375 \nQ 556 841 556 1753 \nL 556 4666 \nz\n\" id=\"DejaVuSans-55\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"63.183594\" xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"113.896484\" xlink:href=\"#DejaVuSans-55\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 36.465625 239.758125 \nL 188.647443 239.758125 \nL 188.647443 140.921761 \nL 36.465625 140.921761 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.440832\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- −2 -->\n      <g transform=\"translate(59.069738 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.556534\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0 -->\n      <g transform=\"translate(109.375284 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.672237\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2 -->\n      <g transform=\"translate(155.490987 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"235.265563\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0 -->\n      <g transform=\"translate(23.103125 239.064782)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"205.31515\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 1 -->\n      <g transform=\"translate(23.103125 209.114369)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"175.364737\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 2 -->\n      <g transform=\"translate(23.103125 179.163955)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md6e7977b10\" y=\"145.414323\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 3 -->\n      <g transform=\"translate(23.103125 149.213542)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#pb76f393926)\" d=\"M 43.38298 235.265563 \nL 112.441052 235.265563 \nL 112.672016 235.115561 \nL 181.730088 145.414323 \nL 181.730088 145.414323 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 36.465625 239.758125 \nL 36.465625 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 188.647443 239.758125 \nL 188.647443 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 36.465625 239.758125 \nL 188.647443 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 36.465625 140.921761 \nL 188.647443 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_21\">\n    <!-- ReLU -->\n    <g transform=\"translate(97.531222 134.921761)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"126.505859\" xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"177.21875\" xlink:href=\"#DejaVuSans-55\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 219.083807 239.758125 \nL 371.265625 239.758125 \nL 371.265625 140.921761 \nL 219.083807 140.921761 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.059013\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- −2 -->\n      <g transform=\"translate(241.68792 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.174716\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0 -->\n      <g transform=\"translate(291.993466 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.290418\" xlink:href=\"#m65a2461b9e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 2 -->\n      <g transform=\"translate(338.109168 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"227.097269\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0 -->\n      <g transform=\"translate(205.721307 230.896487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"199.86962\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 1 -->\n      <g transform=\"translate(205.721307 203.668839)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"172.641972\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 2 -->\n      <g transform=\"translate(205.721307 176.44119)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.083807\" xlink:href=\"#md6e7977b10\" y=\"145.414323\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 3 -->\n      <g transform=\"translate(205.721307 149.213542)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#pa9435a573c)\" d=\"M 226.001162 235.265563 \nL 295.059234 227.110905 \nL 295.521161 226.688172 \nL 364.34827 145.414323 \nL 364.34827 145.414323 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 219.083807 239.758125 \nL 219.083807 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 371.265625 239.758125 \nL 371.265625 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 219.083807 239.758125 \nL 371.265625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 219.083807 140.921761 \nL 371.265625 140.921761 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_29\">\n    <!-- LeakyReLU -->\n    <g transform=\"translate(262.734403 134.921761)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 581 4863 \nL 1159 4863 \nL 1159 1991 \nL 2875 3500 \nL 3609 3500 \nL 1753 1863 \nL 3688 0 \nL 2938 0 \nL 1159 1709 \nL 1159 0 \nL 581 0 \nL 581 4863 \nz\n\" id=\"DejaVuSans-6b\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"115.486328\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"176.765625\" xlink:href=\"#DejaVuSans-6b\"/>\n     <use x=\"231.050781\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"290.230469\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"355.212891\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"416.736328\" xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"467.449219\" xlink:href=\"#DejaVuSans-55\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"paf56b18773\">\n   <rect height=\"98.836364\" width=\"152.181818\" x=\"36.465625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p2a593fe83d\">\n   <rect height=\"98.836364\" width=\"152.181818\" x=\"219.083807\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pb76f393926\">\n   <rect height=\"98.836364\" width=\"152.181818\" x=\"36.465625\" y=\"140.921761\"/>\n  </clipPath>\n  <clipPath id=\"pa9435a573c\">\n   <rect height=\"98.836364\" width=\"152.181818\" x=\"219.083807\" y=\"140.921761\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "actFunc = 'relu'\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, activation=actFunc, input_dim=2))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(10, activation=actFunc))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # output layer uses sigmoid function in order to be in range (0,1)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-18 22:58:08.566073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-18 22:58:08.566341: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-07-18 22:58:12.231164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-18 22:58:12.231234: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-18 22:58:12.231271: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fennecfox38-13UD580-GX30K): /proc/driver/nvidia/version does not exist\n",
      "2021-07-18 22:58:12.231831: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model.fit(x, y, epochs=10000, batch_size=2, verbose=1)\n",
    "model.evaluate(x, y)\n",
    "\n",
    "predicted = model.predict(x)\n",
    "predicted"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " [==============================] - 0s 6ms/step - loss: 4.1242e-06 - accuracy: 1.0000\n",
      "Epoch 9814/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1235e-06 - accuracy: 1.0000\n",
      "Epoch 9815/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1230e-06 - accuracy: 1.0000\n",
      "Epoch 9816/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1225e-06 - accuracy: 1.0000\n",
      "Epoch 9817/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1220e-06 - accuracy: 1.0000\n",
      "Epoch 9818/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1215e-06 - accuracy: 1.0000\n",
      "Epoch 9819/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1209e-06 - accuracy: 1.0000\n",
      "Epoch 9820/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1204e-06 - accuracy: 1.0000\n",
      "Epoch 9821/10000\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 4.1201e-06 - accuracy: 1.0000\n",
      "Epoch 9822/10000\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.1194e-06 - accuracy: 1.0000\n",
      "Epoch 9823/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1189e-06 - accuracy: 1.0000\n",
      "Epoch 9824/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1185e-06 - accuracy: 1.0000\n",
      "Epoch 9825/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1178e-06 - accuracy: 1.0000\n",
      "Epoch 9826/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1174e-06 - accuracy: 1.0000\n",
      "Epoch 9827/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1170e-06 - accuracy: 1.0000\n",
      "Epoch 9828/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1164e-06 - accuracy: 1.0000\n",
      "Epoch 9829/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1158e-06 - accuracy: 1.0000\n",
      "Epoch 9830/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1156e-06 - accuracy: 1.0000\n",
      "Epoch 9831/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1147e-06 - accuracy: 1.0000\n",
      "Epoch 9832/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1142e-06 - accuracy: 1.0000\n",
      "Epoch 9833/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1138e-06 - accuracy: 1.0000\n",
      "Epoch 9834/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1133e-06 - accuracy: 1.0000\n",
      "Epoch 9835/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1128e-06 - accuracy: 1.0000\n",
      "Epoch 9836/10000\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1123e-06 - accuracy: 1.0000\n",
      "Epoch 9837/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1118e-06 - accuracy: 1.0000\n",
      "Epoch 9838/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1113e-06 - accuracy: 1.0000\n",
      "Epoch 9839/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1106e-06 - accuracy: 1.0000\n",
      "Epoch 9840/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1101e-06 - accuracy: 1.0000\n",
      "Epoch 9841/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1097e-06 - accuracy: 1.0000\n",
      "Epoch 9842/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1091e-06 - accuracy: 1.0000\n",
      "Epoch 9843/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1085e-06 - accuracy: 1.0000\n",
      "Epoch 9844/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1080e-06 - accuracy: 1.0000\n",
      "Epoch 9845/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1077e-06 - accuracy: 1.0000\n",
      "Epoch 9846/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1071e-06 - accuracy: 1.0000\n",
      "Epoch 9847/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1065e-06 - accuracy: 1.0000\n",
      "Epoch 9848/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1060e-06 - accuracy: 1.0000\n",
      "Epoch 9849/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1054e-06 - accuracy: 1.0000\n",
      "Epoch 9850/10000\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.1050e-06 - accuracy: 1.0000\n",
      "Epoch 9851/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1046e-06 - accuracy: 1.0000\n",
      "Epoch 9852/10000\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.1041e-06 - accuracy: 1.0000\n",
      "Epoch 9853/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1035e-06 - accuracy: 1.0000\n",
      "Epoch 9854/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1030e-06 - accuracy: 1.0000\n",
      "Epoch 9855/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1025e-06 - accuracy: 1.0000\n",
      "Epoch 9856/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1019e-06 - accuracy: 1.0000\n",
      "Epoch 9857/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1015e-06 - accuracy: 1.0000\n",
      "Epoch 9858/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.1010e-06 - accuracy: 1.0000\n",
      "Epoch 9859/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.1005e-06 - accuracy: 1.0000\n",
      "Epoch 9860/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0999e-06 - accuracy: 1.0000\n",
      "Epoch 9861/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0995e-06 - accuracy: 1.0000\n",
      "Epoch 9862/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0989e-06 - accuracy: 1.0000\n",
      "Epoch 9863/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0983e-06 - accuracy: 1.0000\n",
      "Epoch 9864/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0978e-06 - accuracy: 1.0000\n",
      "Epoch 9865/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0973e-06 - accuracy: 1.0000\n",
      "Epoch 9866/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0969e-06 - accuracy: 1.0000\n",
      "Epoch 9867/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0962e-06 - accuracy: 1.0000\n",
      "Epoch 9868/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0957e-06 - accuracy: 1.0000\n",
      "Epoch 9869/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0952e-06 - accuracy: 1.0000\n",
      "Epoch 9870/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0949e-06 - accuracy: 1.0000\n",
      "Epoch 9871/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0945e-06 - accuracy: 1.0000\n",
      "Epoch 9872/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0937e-06 - accuracy: 1.0000\n",
      "Epoch 9873/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0933e-06 - accuracy: 1.0000\n",
      "Epoch 9874/10000\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.0929e-06 - accuracy: 1.0000\n",
      "Epoch 9875/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0922e-06 - accuracy: 1.0000\n",
      "Epoch 9876/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0916e-06 - accuracy: 1.0000\n",
      "Epoch 9877/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0911e-06 - accuracy: 1.0000\n",
      "Epoch 9878/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0908e-06 - accuracy: 1.0000\n",
      "Epoch 9879/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0902e-06 - accuracy: 1.0000\n",
      "Epoch 9880/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0896e-06 - accuracy: 1.0000\n",
      "Epoch 9881/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0890e-06 - accuracy: 1.0000\n",
      "Epoch 9882/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0885e-06 - accuracy: 1.0000\n",
      "Epoch 9883/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0880e-06 - accuracy: 1.0000\n",
      "Epoch 9884/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0875e-06 - accuracy: 1.0000\n",
      "Epoch 9885/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0872e-06 - accuracy: 1.0000\n",
      "Epoch 9886/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0866e-06 - accuracy: 1.0000\n",
      "Epoch 9887/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0862e-06 - accuracy: 1.0000\n",
      "Epoch 9888/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0855e-06 - accuracy: 1.0000\n",
      "Epoch 9889/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0852e-06 - accuracy: 1.0000\n",
      "Epoch 9890/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0847e-06 - accuracy: 1.0000\n",
      "Epoch 9891/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0840e-06 - accuracy: 1.0000\n",
      "Epoch 9892/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0834e-06 - accuracy: 1.0000\n",
      "Epoch 9893/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0830e-06 - accuracy: 1.0000\n",
      "Epoch 9894/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0824e-06 - accuracy: 1.0000\n",
      "Epoch 9895/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0821e-06 - accuracy: 1.0000\n",
      "Epoch 9896/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0815e-06 - accuracy: 1.0000\n",
      "Epoch 9897/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0811e-06 - accuracy: 1.0000\n",
      "Epoch 9898/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0805e-06 - accuracy: 1.0000\n",
      "Epoch 9899/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0799e-06 - accuracy: 1.0000\n",
      "Epoch 9900/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0794e-06 - accuracy: 1.0000\n",
      "Epoch 9901/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0789e-06 - accuracy: 1.0000\n",
      "Epoch 9902/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0785e-06 - accuracy: 1.0000\n",
      "Epoch 9903/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0779e-06 - accuracy: 1.0000\n",
      "Epoch 9904/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0773e-06 - accuracy: 1.0000\n",
      "Epoch 9905/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0768e-06 - accuracy: 1.0000\n",
      "Epoch 9906/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0763e-06 - accuracy: 1.0000\n",
      "Epoch 9907/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0758e-06 - accuracy: 1.0000\n",
      "Epoch 9908/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0754e-06 - accuracy: 1.0000\n",
      "Epoch 9909/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0748e-06 - accuracy: 1.0000\n",
      "Epoch 9910/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0743e-06 - accuracy: 1.0000\n",
      "Epoch 9911/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0738e-06 - accuracy: 1.0000\n",
      "Epoch 9912/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0733e-06 - accuracy: 1.0000\n",
      "Epoch 9913/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0729e-06 - accuracy: 1.0000\n",
      "Epoch 9914/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0724e-06 - accuracy: 1.0000\n",
      "Epoch 9915/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0718e-06 - accuracy: 1.0000\n",
      "Epoch 9916/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0714e-06 - accuracy: 1.0000\n",
      "Epoch 9917/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0710e-06 - accuracy: 1.0000\n",
      "Epoch 9918/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0703e-06 - accuracy: 1.0000\n",
      "Epoch 9919/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0698e-06 - accuracy: 1.0000\n",
      "Epoch 9920/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0692e-06 - accuracy: 1.0000\n",
      "Epoch 9921/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0687e-06 - accuracy: 1.0000\n",
      "Epoch 9922/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0682e-06 - accuracy: 1.0000\n",
      "Epoch 9923/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0678e-06 - accuracy: 1.0000\n",
      "Epoch 9924/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0673e-06 - accuracy: 1.0000\n",
      "Epoch 9925/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0669e-06 - accuracy: 1.0000\n",
      "Epoch 9926/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0662e-06 - accuracy: 1.0000\n",
      "Epoch 9927/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0657e-06 - accuracy: 1.0000\n",
      "Epoch 9928/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0652e-06 - accuracy: 1.0000\n",
      "Epoch 9929/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0648e-06 - accuracy: 1.0000\n",
      "Epoch 9930/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0642e-06 - accuracy: 1.0000\n",
      "Epoch 9931/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0638e-06 - accuracy: 1.0000\n",
      "Epoch 9932/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0632e-06 - accuracy: 1.0000\n",
      "Epoch 9933/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0627e-06 - accuracy: 1.0000\n",
      "Epoch 9934/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0623e-06 - accuracy: 1.0000\n",
      "Epoch 9935/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0617e-06 - accuracy: 1.0000\n",
      "Epoch 9936/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0612e-06 - accuracy: 1.0000\n",
      "Epoch 9937/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0608e-06 - accuracy: 1.0000\n",
      "Epoch 9938/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0601e-06 - accuracy: 1.0000\n",
      "Epoch 9939/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0597e-06 - accuracy: 1.0000\n",
      "Epoch 9940/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0591e-06 - accuracy: 1.0000\n",
      "Epoch 9941/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0589e-06 - accuracy: 1.0000\n",
      "Epoch 9942/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0582e-06 - accuracy: 1.0000\n",
      "Epoch 9943/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0577e-06 - accuracy: 1.0000\n",
      "Epoch 9944/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0572e-06 - accuracy: 1.0000\n",
      "Epoch 9945/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0566e-06 - accuracy: 1.0000\n",
      "Epoch 9946/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0563e-06 - accuracy: 1.0000\n",
      "Epoch 9947/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0557e-06 - accuracy: 1.0000\n",
      "Epoch 9948/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0551e-06 - accuracy: 1.0000\n",
      "Epoch 9949/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0546e-06 - accuracy: 1.0000\n",
      "Epoch 9950/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0543e-06 - accuracy: 1.0000\n",
      "Epoch 9951/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0538e-06 - accuracy: 1.0000\n",
      "Epoch 9952/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0531e-06 - accuracy: 1.0000\n",
      "Epoch 9953/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0526e-06 - accuracy: 1.0000\n",
      "Epoch 9954/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0521e-06 - accuracy: 1.0000\n",
      "Epoch 9955/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0516e-06 - accuracy: 1.0000\n",
      "Epoch 9956/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0512e-06 - accuracy: 1.0000\n",
      "Epoch 9957/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0506e-06 - accuracy: 1.0000\n",
      "Epoch 9958/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0501e-06 - accuracy: 1.0000\n",
      "Epoch 9959/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0500e-06 - accuracy: 1.0000\n",
      "Epoch 9960/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0492e-06 - accuracy: 1.0000\n",
      "Epoch 9961/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0486e-06 - accuracy: 1.0000\n",
      "Epoch 9962/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0483e-06 - accuracy: 1.0000\n",
      "Epoch 9963/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0477e-06 - accuracy: 1.0000\n",
      "Epoch 9964/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0471e-06 - accuracy: 1.0000\n",
      "Epoch 9965/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0466e-06 - accuracy: 1.0000\n",
      "Epoch 9966/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0462e-06 - accuracy: 1.0000\n",
      "Epoch 9967/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0458e-06 - accuracy: 1.0000\n",
      "Epoch 9968/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0452e-06 - accuracy: 1.0000\n",
      "Epoch 9969/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0447e-06 - accuracy: 1.0000\n",
      "Epoch 9970/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0441e-06 - accuracy: 1.0000\n",
      "Epoch 9971/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0438e-06 - accuracy: 1.0000\n",
      "Epoch 9972/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0432e-06 - accuracy: 1.0000\n",
      "Epoch 9973/10000\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0426e-06 - accuracy: 1.0000\n",
      "Epoch 9974/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0421e-06 - accuracy: 1.0000\n",
      "Epoch 9975/10000\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0418e-06 - accuracy: 1.0000\n",
      "Epoch 9976/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0412e-06 - accuracy: 1.0000\n",
      "Epoch 9977/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0407e-06 - accuracy: 1.0000\n",
      "Epoch 9978/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0401e-06 - accuracy: 1.0000\n",
      "Epoch 9979/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0397e-06 - accuracy: 1.0000\n",
      "Epoch 9980/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0392e-06 - accuracy: 1.0000\n",
      "Epoch 9981/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0386e-06 - accuracy: 1.0000\n",
      "Epoch 9982/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0384e-06 - accuracy: 1.0000\n",
      "Epoch 9983/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0378e-06 - accuracy: 1.0000\n",
      "Epoch 9984/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0372e-06 - accuracy: 1.0000\n",
      "Epoch 9985/10000\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.0368e-06 - accuracy: 1.0000\n",
      "Epoch 9986/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0364e-06 - accuracy: 1.0000\n",
      "Epoch 9987/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0357e-06 - accuracy: 1.0000\n",
      "Epoch 9988/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0353e-06 - accuracy: 1.0000\n",
      "Epoch 9989/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0348e-06 - accuracy: 1.0000\n",
      "Epoch 9990/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0341e-06 - accuracy: 1.0000\n",
      "Epoch 9991/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0336e-06 - accuracy: 1.0000\n",
      "Epoch 9992/10000\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.0331e-06 - accuracy: 1.0000\n",
      "Epoch 9993/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0328e-06 - accuracy: 1.0000\n",
      "Epoch 9994/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0321e-06 - accuracy: 1.0000\n",
      "Epoch 9995/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0317e-06 - accuracy: 1.0000\n",
      "Epoch 9996/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0313e-06 - accuracy: 1.0000\n",
      "Epoch 9997/10000\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0306e-06 - accuracy: 1.0000\n",
      "Epoch 9998/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0301e-06 - accuracy: 1.0000\n",
      "Epoch 9999/10000\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0296e-06 - accuracy: 1.0000\n",
      "Epoch 10000/10000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.0293e-06 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 4.0288e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.30610215e-05],\n",
       "       [9.99998987e-01],\n",
       "       [9.99998748e-01],\n",
       "       [7.25432017e-07]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization\n",
    "\n",
    "(cf. [cs231n.github.io/neural-networks-2](https://cs231n.github.io/neural-networks-2))\n",
    "\n",
    "## Xavier Initialization & He Initialization\n",
    "\n",
    "Initial value might affect the result or rate of convergence if it is not convex enough. The concept of 'Xavier Initialization' is lower varience below specific level.\n",
    "\n",
    "Note that $n_{in}$ is the number of node in previous layer, and $n_{out}$ is the number of node in next layer.\n",
    "\n",
    "Xavier Initialization [[Understanding the difficulty of training deep feedforward neural networks by Glorot et al.,2010]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf):\n",
    "$$W \\sim N(0, Var(W))$$\n",
    "$$Var(W)=\\frac{2}{n_{in}+n_{out}}$$\n",
    "\n",
    "```W = np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)```\n",
    "\n",
    "in keras:\n",
    "```model.add(layers.Dense(fan_out, activation='sigmoid', input_dim=fan_in, kernel_initializer='GlorotNormal'))```\n",
    "\n",
    "Xavier Initialization in ReLU Model might kill half of its node. To compensate those dead nodes, set variance 0.5 times in following He Initialization.\n",
    "\n",
    "He Initialization [[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by He et al.,2015]](https://arxiv.org/abs/1502.01852):\n",
    "$$W \\sim N(0, Var(W))$$\n",
    "$$Var(W)=\\frac{2}{n_{in}}$$\n",
    "\n",
    "```W = np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2)```\n",
    "\n",
    "in keras:\n",
    "```model.add(layers.Dense(fan_out, activation='relu', input_dim=fan_in, kernel_initializer='he_normal'))```\n",
    "\n",
    "\n",
    "\n",
    "## Drop out:\n",
    "![dropout.jpeg](dropout.jpeg)\n",
    "\n",
    "[[Dropout paper by Srivastava et al.,2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)]\n",
    "\n",
    "While training, dropout is implemented by only keeping a neuron active with some probability $p$ (a hyperparameter), or setting it to zero otherwise.\n",
    "\n",
    "in keras (30%):\n",
    "```model.add(layers.Dropout(0.3))```\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "* Distribution on each mini-batch keeps changing.\n",
    "* In order to learn changed distribuition, model deviates a little from correct direction.\n",
    "* **Batch Normalization** prevents the mean of weights on each batch from being biased toward + or -, so that backpropagate weights smoothly.\n",
    "* **Batch Normalization** is the concept of normalizing the output of the middle layer so that the next layer learns normalized output from the previous layer.\n",
    "\n",
    "$$BN(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_B}{\\sqrt{{\\sigma_B}^2 + \\epsilon}}$$\n",
    "\n",
    "in keras: ```model.add(tf.keras.layers.BatchNormalization())```\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Coveriance Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "## Momentum\n",
    "\n",
    "* is the concept of giving momentum to weight when updating so that the previous gradient effects to next gradient.\n",
    "* GDM is normally expressed as: $W:=W-\\lambda \\frac{\\partial Cost(W)}{\\partial W}$\n",
    "* Momentum method works as: $$W:=W-m, \\quad m:=\\beta m + \\lambda \\frac{\\partial Cost(W)}{\\partial W}$$\n",
    "* $m$: moment\n",
    "* $\\beta$: proportion of moment reflected to updates\n",
    "\n",
    "in keras: ```tensorflow.keras.optimizers.SGD(learning_rate=0.01,momentum=0.9)```\n",
    "\n",
    "## Nesterov Momentum\n",
    "* is variation of momentum method.\n",
    "* is known as performing better than the existing momentum method.\n",
    "\n",
    "![NesterovMomentum.png](NesterovMomentum.png)\n",
    "\n",
    "$$W:=W-m, \\quad m:=\\beta m + \\lambda \\frac{\\partial Cost(W-\\beta m)}{\\partial W}$$\n",
    "\n",
    "in keras: ```tensorflow.keras.optimizer.SGD(learning_rate=0.01,momentum=0.9, nesterov=True)```\n",
    "\n",
    "## AdaGrad\n",
    "* is the method that change the weight of a variable with **gradual slope more than a steep slope**.\n",
    "* is not preferred for deep learning model because it tends to terminate too early.\n",
    "\n",
    "\n",
    "$$G_t = G_{t-1} + (\\nabla_{\\theta} J(\\theta_t))^2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta_t)$$\n",
    "\n",
    "\n",
    "* $G_t$ contains square sum of change in each variable at time t.\n",
    "* When updating weight, put change in denominator so that large changes are adjusted to be small.\n",
    "\n",
    "![AdaGrad.png](AdaGrad.png)\n",
    "\n",
    "in keras: ```tensorflow.keras.optimizers.Adagrad( learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07 )```\n",
    "\n",
    "## RMSProp\n",
    "* is suggested by prof. Hinton to complement shortcoming of AdaGrad.\n",
    "* does not accumulate the square sum of change, but calculates exponential mean.\n",
    "* reflect the recent change significantly, making them more accurate in the time series.\n",
    "\n",
    "\n",
    "$$G := \\gamma G + (1-\\gamma) (\\nabla_{\\theta} J(\\theta_t))^2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\theta := \\theta - \\frac{\\eta}{\\sqrt{G+\\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta_t)$$\n",
    "\n",
    "\n",
    "in keras:\n",
    "```python\n",
    "tensorflow.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.9, # Discounting factor for the history/coming gradient. Defaults to 0.9.\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07, # A small constant for numerical stability\n",
    "    centered=False, # If True, gradients are normalized by the estimated variance of the gradient; if False, by the uncentered second moment.\n",
    ")\n",
    "```\n",
    "\n",
    "## Adam Optimizer\n",
    "* is concept of combination of momentum and RMSProp.\n",
    "* is most widely used.\n",
    "\n",
    "in keras:\n",
    "```python\n",
    "tensorflow.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9, # the exponential decay rate for the 1st moment extimates.\n",
    "    beta_2=0.999, # the exponential decay rate for the 2nd moment extimates.\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    ")\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# load MNIST Datasets\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "mnist = datasets.mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "train_x = train_x.reshape(-1,784) \n",
    "test_x = test_x.reshape(-1,784)\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "train_y_onehot = to_categorical(train_y)\n",
    "test_y_onehot = to_categorical(test_y)\n",
    "\n",
    "train_x.shape,train_y_onehot[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000, 784), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Add hidden layer (using relu and softmax)\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=784))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y_onehot, batch_size = 100, epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 12:45:30.263276: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-19 12:45:30.263939: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-19 12:45:30.264455: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fennecfox38-13UD580-GX30K): /proc/driver/nvidia/version does not exist\n",
      "2021-07-19 12:45:30.266644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-19 12:45:31.615999: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2021-07-19 12:45:32.328241: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-19 12:45:32.350845: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2400000000 Hz\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 5s 6ms/step - loss: 1.0422 - accuracy: 0.7592\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.4232 - accuracy: 0.8875\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3429 - accuracy: 0.9049\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.3056 - accuracy: 0.9140\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 3s 5ms/step - loss: 0.2800 - accuracy: 0.9215\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c60de5e80>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Use Adam Optimizer instead of sgd\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=784))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y_onehot, batch_size = 100, epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 12:46:29.431325: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 6s 9ms/step - loss: 0.2412 - accuracy: 0.9297\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0896 - accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 6s 9ms/step - loss: 0.0592 - accuracy: 0.9815\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0439 - accuracy: 0.9860\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0337 - accuracy: 0.9893\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c74e65a00>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Use Adam Optimizer with He initializer\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=784, kernel_initializer='he_normal'))\n",
    "model.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(layers.Dense(10, activation='softmax', kernel_initializer='he_normal'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y_onehot, batch_size = 100, epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 12:47:04.332385: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.2378 - accuracy: 0.9304\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0923 - accuracy: 0.9720\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0599 - accuracy: 0.9808\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.0433 - accuracy: 0.9865\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0312 - accuracy: 0.9905\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c74563610>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Adam Optimizer + He initializer + droupout\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal', input_dim=784))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_x, train_y_onehot, batch_size = 100, epochs=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 12:48:24.557358: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "Epoch 1/10\n",
      "600/600 [==============================] - 7s 9ms/step - loss: 0.3330 - accuracy: 0.8984\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1517 - accuracy: 0.9544\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.1108 - accuracy: 0.9660\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0918 - accuracy: 0.9721\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0797 - accuracy: 0.9746\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0707 - accuracy: 0.9776\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0628 - accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0560 - accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0522 - accuracy: 0.9835\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0488 - accuracy: 0.9839\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8c744608b0>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model.evaluate(test_x,test_y_onehot)\n",
    "model.save(\"mnist_test.h5\") # saving model\n",
    "!ls"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.9805\n",
      "dropout.jpeg\t  mnist_test.h5        NeuronCell.png\t VanishingGradient.png\n",
      "HiddenLayer1.png  NeuralModel.png      or_and_xor.png\n",
      "HiddenLayer2.png  NeuralNetwork.ipynb  Tensorflow.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# reload saved model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mnist = datasets.mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "train_x = train_x.reshape(-1,784) \n",
    "test_x = test_x.reshape(-1,784)\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "train_y_onehot = to_categorical(train_y)\n",
    "test_y_onehot = to_categorical(test_y)\n",
    "train_y_onehot[0]\n",
    "\n",
    "model = tf.keras.models.load_model('mnist_test.h5')\n",
    "model.evaluate(test_x, test_y_onehot)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0679 - accuracy: 0.9805\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.06793633103370667, 0.9804999828338623]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}