{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression Model using Tensorflow(Gradient Descent)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,3])\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(1, activation='linear', input_dim=1)) # output:1 | Activation Function: linear | input:1\n",
    "sgd=tf.keras.optimizers.SGD(0.01) # Learning Rate = 0.01\n",
    "model.compile(optimizer=sgd,loss='mse') # mse = (SSE)/(n)\n",
    "model.fit(x, y, epochs=1000, verbose = 1) #1000 times | verbose=0 not to print"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "p - loss: 1.5430e-04\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5356e-04\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5282e-04\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5209e-04\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5136e-04\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5063e-04\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4991e-04\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4919e-04\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4847e-04\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4776e-04\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4705e-04\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4634e-04\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4564e-04\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4494e-04\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4424e-04\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4355e-04\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4286e-04\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4218e-04\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4149e-04\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4081e-04\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4014e-04\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3946e-04\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3879e-04\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3813e-04\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3746e-04\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3680e-04\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3615e-04\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3549e-04\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3484e-04\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3419e-04\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3355e-04\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3291e-04\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3227e-04\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3164e-04\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3101e-04\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3037e-04\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2975e-04\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2913e-04\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2851e-04\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2789e-04\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2727e-04\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2666e-04\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2605e-04\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2545e-04\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2485e-04\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2425e-04\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2365e-04\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2306e-04\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2247e-04\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2188e-04\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2129e-04\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2071e-04\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2013e-04\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1955e-04\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1898e-04\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1841e-04\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1784e-04\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1727e-04\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1671e-04\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1615e-04\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1559e-04\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1504e-04\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1449e-04\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1394e-04\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1339e-04\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1284e-04\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1230e-04\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1176e-04\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1123e-04\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1069e-04\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1016e-04\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0963e-04\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0910e-04\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0858e-04\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0806e-04\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0754e-04\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0703e-04\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0651e-04\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0600e-04\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0549e-04\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0498e-04\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0448e-04\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0398e-04\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0348e-04\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0298e-04\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0249e-04\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0200e-04\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0150e-04\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0102e-04\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0053e-04\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0005e-04\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.9569e-05\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9091e-05\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8615e-05\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8142e-05\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7669e-05\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7202e-05\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6736e-05\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6270e-05\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.5809e-05\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5349e-05\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4889e-05\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4434e-05\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.3981e-05\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3531e-05\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3081e-05\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.2633e-05\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.2189e-05\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1747e-05\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.1307e-05\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.0868e-05\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.0431e-05\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9996e-05\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9564e-05\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9133e-05\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8707e-05\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8280e-05\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7858e-05\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7435e-05\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7014e-05\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6596e-05\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.6180e-05\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5767e-05\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5355e-05\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.4945e-05\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4537e-05\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4132e-05\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3727e-05\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3326e-05\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2926e-05\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.2528e-05\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2131e-05\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.1737e-05\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1344e-05\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0954e-05\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0564e-05\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0178e-05\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.9793e-05\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.9410e-05\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.9029e-05\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.8649e-05\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8271e-05\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7896e-05\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7521e-05\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7149e-05\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6778e-05\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6410e-05\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.6043e-05\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5678e-05\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5314e-05\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4953e-05\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.4593e-05\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4235e-05\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.3877e-05\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3524e-05\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.3170e-05\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2819e-05\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2469e-05\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2122e-05\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1775e-05\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1431e-05\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1088e-05\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0746e-05\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0406e-05\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0068e-05\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9732e-05\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9396e-05\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9063e-05\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8731e-05\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8402e-05\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8073e-05\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7745e-05\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7421e-05\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7098e-05\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.6775e-05\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6453e-05\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6135e-05\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5818e-05\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5502e-05\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5187e-05\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4875e-05\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4563e-05\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4252e-05\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3944e-05\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3637e-05\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.3331e-05\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.3027e-05\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.2724e-05\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2424e-05\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2123e-05\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.1824e-05\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1528e-05\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1232e-05\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0939e-05\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0646e-05\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0355e-05\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.0066e-05\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.9776e-05\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9490e-05\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9204e-05\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8920e-05\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8637e-05\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8355e-05\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8074e-05\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7796e-05\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.7518e-05\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.7243e-05\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6967e-05\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6693e-05\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6421e-05\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6150e-05\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5881e-05\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5613e-05\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5346e-05\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5078e-05\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.4816e-05\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4552e-05\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4290e-05\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4029e-05\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3769e-05\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3512e-05\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3254e-05\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2999e-05\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2745e-05\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2491e-05\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2239e-05\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1989e-05\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1739e-05\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1491e-05\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1244e-05\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0997e-05\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0752e-05\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.0509e-05\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0265e-05\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0025e-05\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9784e-05\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c7f614f40>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "predicted = model.predict(x)\n",
    "predicted"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.98959106],\n",
       "       [1.9977663 ],\n",
       "       [3.0059414 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Model using Tensorflow(Gradient Descent)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x=np.array([[1],[2],[3],[4],[5],[6]])\n",
    "y = np.array([[0],[0],[0],[1],[1],[1]])\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(1, activation='sigmoid', input_dim=1)) # output:1 | Activation Function: Sigmoid | input:1\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(0.1),loss='binary_crossentropy') # loss function (cost function): binary_cross_entropy\n",
    "model.fit(x, y, epochs=2000, verbose = 1) # 1 datasets * 2000 epochs => 2000 updates"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1147\n",
      "Epoch 1758/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1147\n",
      "Epoch 1759/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1146\n",
      "Epoch 1760/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1146\n",
      "Epoch 1761/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1146\n",
      "Epoch 1762/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1145\n",
      "Epoch 1763/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1145\n",
      "Epoch 1764/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1145\n",
      "Epoch 1765/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1144\n",
      "Epoch 1766/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1144\n",
      "Epoch 1767/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1144\n",
      "Epoch 1768/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1143\n",
      "Epoch 1769/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1143\n",
      "Epoch 1770/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1143\n",
      "Epoch 1771/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1142\n",
      "Epoch 1772/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1142\n",
      "Epoch 1773/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1142\n",
      "Epoch 1774/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1141\n",
      "Epoch 1775/2000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1141\n",
      "Epoch 1776/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1141\n",
      "Epoch 1777/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1140\n",
      "Epoch 1778/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1140\n",
      "Epoch 1779/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1140\n",
      "Epoch 1780/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1139\n",
      "Epoch 1781/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1139\n",
      "Epoch 1782/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1139\n",
      "Epoch 1783/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1138\n",
      "Epoch 1784/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1138\n",
      "Epoch 1785/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1138\n",
      "Epoch 1786/2000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1137\n",
      "Epoch 1787/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1137\n",
      "Epoch 1788/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1137\n",
      "Epoch 1789/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1136\n",
      "Epoch 1790/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1136\n",
      "Epoch 1791/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1136\n",
      "Epoch 1792/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1135\n",
      "Epoch 1793/2000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1135\n",
      "Epoch 1794/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1135\n",
      "Epoch 1795/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1134\n",
      "Epoch 1796/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1134\n",
      "Epoch 1797/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1134\n",
      "Epoch 1798/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1133\n",
      "Epoch 1799/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1133\n",
      "Epoch 1800/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 1801/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1132\n",
      "Epoch 1802/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1132\n",
      "Epoch 1803/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1132\n",
      "Epoch 1804/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1132\n",
      "Epoch 1805/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1131\n",
      "Epoch 1806/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1131\n",
      "Epoch 1807/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1131\n",
      "Epoch 1808/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1130\n",
      "Epoch 1809/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1130\n",
      "Epoch 1810/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1130\n",
      "Epoch 1811/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1129\n",
      "Epoch 1812/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1129\n",
      "Epoch 1813/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1129\n",
      "Epoch 1814/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1128\n",
      "Epoch 1815/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1128\n",
      "Epoch 1816/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1128\n",
      "Epoch 1817/2000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1127\n",
      "Epoch 1818/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1127\n",
      "Epoch 1819/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1127\n",
      "Epoch 1820/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1126\n",
      "Epoch 1821/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1126\n",
      "Epoch 1822/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1126\n",
      "Epoch 1823/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 1824/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1125\n",
      "Epoch 1825/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1125\n",
      "Epoch 1826/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1124\n",
      "Epoch 1827/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1124\n",
      "Epoch 1828/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1124\n",
      "Epoch 1829/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1124\n",
      "Epoch 1830/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1123\n",
      "Epoch 1831/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1123\n",
      "Epoch 1832/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1123\n",
      "Epoch 1833/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1122\n",
      "Epoch 1834/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1122\n",
      "Epoch 1835/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1122\n",
      "Epoch 1836/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1121\n",
      "Epoch 1837/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1121\n",
      "Epoch 1838/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1121\n",
      "Epoch 1839/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1120\n",
      "Epoch 1840/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1120\n",
      "Epoch 1841/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1120\n",
      "Epoch 1842/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1119\n",
      "Epoch 1843/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1119\n",
      "Epoch 1844/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1119\n",
      "Epoch 1845/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1119\n",
      "Epoch 1846/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1118\n",
      "Epoch 1847/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1118\n",
      "Epoch 1848/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1118\n",
      "Epoch 1849/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1117\n",
      "Epoch 1850/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 1851/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1117\n",
      "Epoch 1852/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1116\n",
      "Epoch 1853/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1116\n",
      "Epoch 1854/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1116\n",
      "Epoch 1855/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1115\n",
      "Epoch 1856/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1115\n",
      "Epoch 1857/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1115\n",
      "Epoch 1858/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1114\n",
      "Epoch 1859/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1114\n",
      "Epoch 1860/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1114\n",
      "Epoch 1861/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 1862/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1113\n",
      "Epoch 1863/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1113\n",
      "Epoch 1864/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 1865/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1112\n",
      "Epoch 1866/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1112\n",
      "Epoch 1867/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1112\n",
      "Epoch 1868/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1111\n",
      "Epoch 1869/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1111\n",
      "Epoch 1870/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1111\n",
      "Epoch 1871/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1111\n",
      "Epoch 1872/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1110\n",
      "Epoch 1873/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1110\n",
      "Epoch 1874/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1110\n",
      "Epoch 1875/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 1876/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 1877/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1109\n",
      "Epoch 1878/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 1879/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 1880/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 1881/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "Epoch 1882/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "Epoch 1883/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1107\n",
      "Epoch 1884/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "Epoch 1885/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1106\n",
      "Epoch 1886/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1106\n",
      "Epoch 1887/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1106\n",
      "Epoch 1888/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 1889/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1105\n",
      "Epoch 1890/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 1891/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1104\n",
      "Epoch 1892/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1104\n",
      "Epoch 1893/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1104\n",
      "Epoch 1894/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1104\n",
      "Epoch 1895/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1103\n",
      "Epoch 1896/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1103\n",
      "Epoch 1897/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1103\n",
      "Epoch 1898/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1102\n",
      "Epoch 1899/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1102\n",
      "Epoch 1900/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1102\n",
      "Epoch 1901/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1101\n",
      "Epoch 1902/2000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1101\n",
      "Epoch 1903/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 1904/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1101\n",
      "Epoch 1905/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1100\n",
      "Epoch 1906/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 1907/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1100\n",
      "Epoch 1908/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1099\n",
      "Epoch 1909/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 1910/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1099\n",
      "Epoch 1911/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1098\n",
      "Epoch 1912/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 1913/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 1914/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 1915/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1097\n",
      "Epoch 1916/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1097\n",
      "Epoch 1917/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1097\n",
      "Epoch 1918/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1096\n",
      "Epoch 1919/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 1920/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1096\n",
      "Epoch 1921/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 1922/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 1923/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 1924/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1095\n",
      "Epoch 1925/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1094\n",
      "Epoch 1926/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1094\n",
      "Epoch 1927/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1094\n",
      "Epoch 1928/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 1929/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 1930/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1093\n",
      "Epoch 1931/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 1932/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 1933/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1092\n",
      "Epoch 1934/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1092\n",
      "Epoch 1935/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1091\n",
      "Epoch 1936/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1091\n",
      "Epoch 1937/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1091\n",
      "Epoch 1938/2000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1091\n",
      "Epoch 1939/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1090\n",
      "Epoch 1940/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1090\n",
      "Epoch 1941/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1090\n",
      "Epoch 1942/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1089\n",
      "Epoch 1943/2000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1089\n",
      "Epoch 1944/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1089\n",
      "Epoch 1945/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1088\n",
      "Epoch 1946/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1088\n",
      "Epoch 1947/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1088\n",
      "Epoch 1948/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1088\n",
      "Epoch 1949/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 1950/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1087\n",
      "Epoch 1951/2000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1087\n",
      "Epoch 1952/2000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1086\n",
      "Epoch 1953/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1086\n",
      "Epoch 1954/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1086\n",
      "Epoch 1955/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1086\n",
      "Epoch 1956/2000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1085\n",
      "Epoch 1957/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 1958/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 1959/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1084\n",
      "Epoch 1960/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1084\n",
      "Epoch 1961/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1084\n",
      "Epoch 1962/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1084\n",
      "Epoch 1963/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 1964/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1083\n",
      "Epoch 1965/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1083\n",
      "Epoch 1966/2000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1082\n",
      "Epoch 1967/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1082\n",
      "Epoch 1968/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1082\n",
      "Epoch 1969/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1082\n",
      "Epoch 1970/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1081\n",
      "Epoch 1971/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1081\n",
      "Epoch 1972/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 1973/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 1974/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 1975/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 1976/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 1977/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 1978/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1079\n",
      "Epoch 1979/2000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 1980/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1078\n",
      "Epoch 1981/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 1982/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1078\n",
      "Epoch 1983/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1078\n",
      "Epoch 1984/2000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1077\n",
      "Epoch 1985/2000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1077\n",
      "Epoch 1986/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 1987/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1077\n",
      "Epoch 1988/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1076\n",
      "Epoch 1989/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 1990/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 1991/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1075\n",
      "Epoch 1992/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 1993/2000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1075\n",
      "Epoch 1994/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 1995/2000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 1996/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1074\n",
      "Epoch 1997/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1074\n",
      "Epoch 1998/2000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1073\n",
      "Epoch 1999/2000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1073\n",
      "Epoch 2000/2000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1073\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c7d3f6040>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "predicted = model.predict(x)\n",
    "predicted"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00435549],\n",
       "       [0.04025552],\n",
       "       [0.2868157 ],\n",
       "       [0.7940674 ],\n",
       "       [0.9736643 ],\n",
       "       [0.9971869 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Database \n",
    "## Modified National Institute of Standards and Technology Database\n",
    "\n",
    "* is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "* was created by \"re-mixing\" the samples from NIST's original datasets.\n",
    "* contains 60,000 training images and 10,000 testing images.\n",
    "* the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "* In module ```tensorflow.keras.datasets.mnist```\n",
    "\n",
    "![MnistExamples.png](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "from [wikipedia](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "\n",
    "Make logistic model using tensorflow\n",
    "* One picture has 28x28=784 pixels. Serialize to 784 independent variables.\n",
    "* Each pixel has value between 0~255. Normalize in range 0~1.\n",
    "* Target(train_y and test_y) has value between 0~9. Perform categorical encoding (One-hot Encoding)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Figure out the shape of train and test set.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "mnist = datasets.mnist # Load MNIST Database\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data() # split to train set and test set.\n",
    "print(\"train_x shape:\",train_x.shape)\n",
    "\n",
    "image = train_x[0]\n",
    "print(\"train_x[0] shape:\",image.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image, 'gray')\n",
    "plt.show()\n",
    "\n",
    "print(train_y.shape)\n",
    "label = train_y[0]\n",
    "print(\"train_y[0] value:\",label)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"train_y[0] is one-hot encoded to:\",to_categorical(label, 10) )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_x shape: (60000, 28, 28)\n",
      "train_x[0] shape: (28, 28)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-07-17T20:27:06.720154</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p0b23662b56)\">\n    <image height=\"218\" id=\"image912b073059\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"maf41be939f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#maf41be939f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mc5005508f5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc5005508f5\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0b23662b56\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000,)\n",
      "train_y[0] value: 5\n",
      "train_y[0] is one-hot encoded to: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Reshape and perform one-hot encoding.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "mnist = datasets.mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "print(\"train_x.shape:\",train_x.shape, \"train_y[0]:\", train_y[0])\n",
    "\n",
    "train_x = train_x.reshape(-1,784) # (60000,784) Serialize 28*28 data to 784\n",
    "test_x = test_x.reshape(-1,784) # -1 means unknown dimension: numpy automatically figures out.\n",
    "train_x = train_x / 255 # pixel has value between 0~255\n",
    "test_x = test_x / 255 # normalize pixel value\n",
    "\n",
    "train_y_onehot = to_categorical(train_y) # y has literal integer values\n",
    "test_y_onehot = to_categorical(test_y) # one-hot encoding\n",
    "\n",
    "print(\"Serialized, reshaped and one-hot encoded.\")\n",
    "\n",
    "print(\"train_x.shape:\",train_x.shape, \"train_y_onehot[0]:\", train_y_onehot[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_x.shape: (60000, 28, 28) train_y[0]: 5\n",
      "Serialized, reshaped and one-hot encoded.\n",
      "train_x.shape: (60000, 784) train_y_onehot[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Add layer and make it fit (Learning)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(10, activation='softmax', input_dim=784)) # output: 10 (0~9) | Activation Function: Softmax | input:784 (Serialized 28*28)\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy']) # Loss Function: Categorical Cross Entropy\n",
    "model.fit(train_x, train_y_onehot, epochs=5) # 5 epochs, default batch_size 32, 60000 datasets => 60000/32 * 5 = 1875*5 updates"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-17 20:27:26.663071: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7776 - accuracy: 0.8141\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4560 - accuracy: 0.8813\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4033 - accuracy: 0.8913\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3768 - accuracy: 0.8970\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3602 - accuracy: 0.9008\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c52ed7610>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Evaluate and predict\n",
    "\n",
    "print(\"Evaluation(Loss, Metric(accuracy)):\",model.evaluate(test_x, test_y_onehot))\n",
    "\n",
    "predicted = model.predict(test_x)\n",
    "print(\"predited[0]:\",predicted[0])\n",
    "print(\"test_y_onehot[0]:\", test_y_onehot[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.9084\n",
      "Evaluation(Loss, Metric(accuracy)): [0.3375749886035919, 0.9083999991416931]\n",
      "predited[0]: [1.9096737e-04 9.7606653e-07 1.6131661e-04 1.6194476e-03 3.6754835e-05\n",
      " 7.9697136e-05 2.4661099e-06 9.9468118e-01 1.9758838e-04 3.0295441e-03]\n",
      "test_y_onehot[0]: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **※ Categorical Encoding ※**\n",
    "* Indexing non-numerical data to certain number.\n",
    "* Converting non-numerical data to numeric data.\n",
    "\n",
    "1. Label Encoding: Indexing non-numerical datas to numbers in order which the original datas have.\n",
    "2. **One-hot Encoding**: Give each category each bit.\n",
    "(Giving normal integer (Label Encoding) might cause learning false relationship between non-related variables)\n",
    "\n",
    "ex)\n",
    "$$\n",
    "Y= \\begin{pmatrix} 0 \\\\ 1 \\\\ 5 \\\\ 9 \\end{pmatrix} \\quad\n",
    "\\xrightarrow{One-hot \\, Encoding} \\quad\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then, hypothesis $H(X)$ would be in same size of matrix like example following:\n",
    "$$\n",
    "H(X)= \n",
    "\\begin{pmatrix}\n",
    "0.7 & 0.2 & 0.1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0.9 & 0 & 0 & 0 & 0 & 0 & 0.1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0.2 & 0 & 0.5 & 0 & 0 & 0.3 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.1 & 0.9 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Categorical Cross Entropy function:\n",
    "$$Cost(W)=\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^c -y_{ij} ln(H(X)_{ij})$$\n",
    "\n",
    "Categorical cross entropy function above can be derived from binary cross entropy function as below:\n",
    "$$Cost(W)=\\sum \\limits_{i=1}^n \\big[ -y_iln(H(X)) + (1-y_i) -ln(1-H(X)) \\big]$$\n",
    "$$=\\sum \\limits_{i=1}^n \\big[ -y_{i1}ln(H(X)) + (1-y_{i1}) -ln(1-H(X)) \\big]$$\n",
    "$$=\\sum \\limits_{i=1}^n \\big[ -y_{i1}ln(H(X)) + y_{i2} -ln(1-H(X)) \\big]$$\n",
    "$$=\\sum \\limits_{i=1}^n \\sum \\limits_{j=1}^c -y_{ij} ln(H(X)_{ij})$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax Function\n",
    "\n",
    "* is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
    "\n",
    "Each one-hot encoded category is affect to each other: only one category can be '1', and elses must be '0'. (e.g. \\[0, 1, 1, 0, 0, 1, 0\\] is illegal because it indicates three categories in one case.)\n",
    "\n",
    "But if normal activation function (e.g. sigmoid) was given, then it calculates its probability independently.\n",
    "\n",
    "Softmax Function for example above (MNIST Database) might be defined as follow:\n",
    "\n",
    "$$\n",
    "P_{sigmoid}(Y=0)=\\frac{1}{1+exp(-b_0 - w_{1,1} x_1 - \\cdots - w_{784,1} x_{784})} \\\\\n",
    "P_{sigmoid}(Y=1)=\\frac{1}{1+exp(-b_1 - w_{1,2} x_1 - \\cdots - w_{784,2} x_{784})} \\\\\n",
    "\\vdots \\\\\n",
    "P_{sigmoid}(Y=9)=\\frac{1}{1+exp(-b_0 - w_{1,10} x_1 - \\cdots - w_{784,10} x_{784})}\\\\\n",
    "$$\n",
    "\n",
    "In order to satisfy $P(Y=0)+P(Y=1)+\\cdots+P(Y=9)=1$, define as\n",
    "\n",
    "$$\n",
    "P(Y=0)=\\frac{P_{sigmoid}(Y=0)}{P_{sigmoid}(Y=0)+P_{sigmoid}(Y=1)+\\cdots+P_{sigmoid}(Y=9)} \\\\\n",
    "P(Y=1)=\\frac{P_{sigmoid}(Y=1)}{P_{sigmoid}(Y=0)+P_{sigmoid}(Y=1)+\\cdots+P_{sigmoid}(Y=9)} \\\\\n",
    "\\vdots \\\\\n",
    "P(Y=9)=\\frac{P_{sigmoid}(Y=9)}{P_{sigmoid}(Y=0)+P_{sigmoid}(Y=1)+\\cdots+P_{sigmoid}(Y=9)}\\\\\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SGD: Stochastic Gradient Descent\n",
    "\n",
    "* is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).\n",
    "* can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (**calculated from a randomly selected subset of the data**).\n",
    "* Especially in high-dimensional optimization problems **this reduces the computational burden, achieving faster iterations in trade for a lower convergence rate**. (surface would be distorted roughly due to fewer sample case)\n",
    "* **Lower convergence rate might lead potential danger of falling into local minimum** (not global)\n",
    "\n",
    "pseudocode:\n",
    "* Choose an initial vector of parameters $w$ and learning rate $\\eta$ .\n",
    "* Repeat until an approximate minimum is obtained:\n",
    "    * Randomly shuffle examples in the training set.\n",
    "    * For $i=1,2,\\cdots,n$, do:\n",
    "        * $w:=w-\\eta \\nabla Q_{i}(w)$\n",
    "\n",
    "from [wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "# BGD: Batch Gradient Descent\n",
    "* implemented **multiple 'Batch'** on SGD.\n",
    "* is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces\n",
    "* Multiple Batchs gives easier parallel processing on multiple GPU\n",
    "\n",
    "[shuuki4.github.io](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)\n",
    "\n",
    "[ruder.io](https://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "# Epochs\n",
    "* One epoch is **when an entire dataset is passed forward and backward through the neural network only once**.\n",
    "\n",
    "# Batch Size\n",
    "* Total number of training examples present in a single batch\n",
    "* $(Size \\, of \\, Dataset) \\div (Batch \\, Size) = (Number \\, of \\, Batchs)$\n",
    "* default batch size is 32\n",
    "\n",
    "\n",
    "e.g.) Size of Dataset = 60000, Batch Size = 32, Epochs = 5\n",
    "\n",
    "Size of one batch is 32 and there are 1875 batchs (=60000/32).\n",
    "\n",
    "One epoch is when all 1875 batchs (each batch includes 32 data) are run so that all 60000 data are learned.\n",
    "\n",
    "If it learns in 5 epochs, then weight would be updated 1875 * 5 = 9375 times.\n",
    "\n",
    "[towardsdatascience.com](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}